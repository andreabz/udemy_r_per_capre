[
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "Numero di campioni e tempi di analisi per il metodo CHEM5421c nell’anno 2024",
    "section": "",
    "text": "Introduzione\nIn questa esercitazione analizzeremo il numero di campioni e i tempi di analisi relativi a un metodo analitico specifico, nell’arco di un intero anno.\nL’obiettivo è duplice:\n\nprendere confidenza con un flusso di lavoro tipico in R (download dei dati, pulizia, trasformazioni, aggregazioni);\nprodurre una sintesi grafica che permetta di valutare l’andamento temporale del carico di lavoro e delle prestazioni analitiche.\n\nIl documento è parametrizzato: modificando i parametri method, target e year è possibile riutilizzare lo stesso codice per analizzare metodi, con obiettivi diversi in anni diversi, senza dover intervenire manualmente sul resto del codice.\n\n\nLibrerie utilizzate\nIniziamo caricando le librerie necessarie. Useremo:\n\ndata.table per la gestione efficiente dei dati tabellari;\nggplot2 per la produzione dei grafici;\npatchwork per combinare più grafici in un’unica figura.\n\nNel resto dell’esercitazione vedremo passo per passo il significato dei vari comandi, cercando di introdurre tutti i concetti che ci serviranno, man mano che li incontreremo.\n\nlibrary(readxl)      # carica dati da file excel\nlibrary(data.table)  # manipola i dati\nlibrary(ggplot2)     # crea grafici\nlibrary(patchwork)   # unisce grafici\nlibrary(knitr)       # genera documenti e tabelle\n\n\n\nDownload dei dati\nUtilizzeremo dati simulati contenuti in due dataset:\n\ndati_campioni.xlsx è un foglio di calcolo contenente i dati relativi a misure effettuate in un laboratorio chimico con tre metodi differenti;\ncomuni_al.csv è un file di testo con valori separati da virgole contenente informazioni sui comuni in provincia di Alessandria.\n\nI due dataset sono in relazione attraverso i punti di prelievo dei campioni: un comune è associato a uno o più punti di prelievo, i quali sono associati a più campioni.\n\nmisure &lt;- read_excel(\"data/dati_campioni.xlsx\") # leggo l'excel con readxl\nmisure &lt;- data.table(misure)                    # converto a data.table\nelenco_comuni &lt;- fread(\"data/comuni_al.csv\")    # anche con read.csv(), senza librerie esterne\n\n\n\nEsplorazione e pulizia dei dati\nDopo un primo controllo della struttura dei dati (ad esempio con str()), emergono alcune criticità tipiche dei dataset reali:\n\nle colonne di intestazioni contengono spazi e maiuscole;\nla data di campionamento è memorizzata come caratteri e non come oggetti Date;\nla colonna dei valori misurati è stata riportata come carattere;\ni valori e le note mancanti non sono correttamente codificate;\ni valori sono memorizzati come testo e non come numeri.\n\nIn questa sezione sistemiamo questi aspetti, rendendo il dataset coerente e pronto per le analisi successive. Questo passaggio è fondamentale: una buona pulizia dei dati semplifica enormemente le analisi successive e riduce il rischio di errori silenziosi, cioè errori che non producono messaggi ma risultati sbagliati.\n\n# dopo str() vedo che alcune cose non funzionano:\n# - le intestazioni contengono caratteri scomodi\n# - le date non sono date\n# - L maiuscola di litri, non si può vedere\n# - i valori mancanti non sono correttamente codificati\n# - i numeri non sono numeri\n\n# intestazioni minuscole e pulite\ncolnames(misure) &lt;- gsub(pattern = \" \", replacement = \"_\", colnames(misure))\ncolnames(misure) &lt;- trimws(colnames(misure))\ncolnames(misure) &lt;- colnames(misure) |&gt; tolower()\n\n# date come date e non come carattere\nmisure[, `:=` (\n  data_campionamento = as.IDate(data_campionamento, format = \"%d/%m/%Y\"), # anche con as.Date e altri modi\n  inizio_analisi = as.IDate(data_inizio_analisi),\n  fine_analisi = as.IDate(data_fine_analisi),\n  data_inizio_analisi = NULL,\n  data_fine_analisi = NULL\n  )]\n\n# converto la L in l in udm\nmisure[, unita_di_misura := gsub(\"L\", \"l\", unita_di_misura)]\n\n# identifico i valori mancanti\nmisure[valore == \"valore mancante\", valore := NA]\nmisure[note == \"-\", note := NA]\n\n# identifico i valori inferiori al limte di quantificazione:\n# sono numeri positivi di cui conosco solo il limite superiore\n# a seconda delle esigenze vengono sostituiti con un valore\n# in questo caso usiamo la metà del limite di quantificazione\n# e creo una colonna identificativa con\n#    - FALSE per i valori inferiori al limite di quantificazione\n#    - TRUE per i valori superiori al limite di quantificazione\nmisure[, quantificato := !grepl(\"^&lt;\", valore)]\nmisure[quantificato == FALSE, valore := limite_quantificazione / 2]\n\n# finalmente converto a numeri\nmisure[, valore := as.numeric(valore)]\n\n\n\nCalcolo delle variabili di interesse\nUna volta che i dati sono in ordine, costruiamo alcune variabili derivate:\n\nil mese di inizio dell’analisi, utile per le aggregazioni temporali\nil tempo di analisi, definito come la differenza tra la data di fine e la data di inizio analisi;\nil tempo di processamento complessivo, calcolato come differenza tra la data di fine e la data di campionamento.\n\nQueste variabili ci permetteranno di studiare sia il volume di lavoro sia la distribuzione dei tempi nel corso dell’anno.\n\n# voglio vedere l'andamento mensile del numero di campioni e tempi di analisi:\n# - calcolo i mesi\n# - calcolo i tempi di analisi\n# - calcolo i tempi di processamento\n\nmisure[, mese_inizio_analisi := month(inizio_analisi)]\nmisure[, tempo_processamento := fine_analisi - data_campionamento]\nmisure[, tempo_analisi := fine_analisi - inizio_analisi]\n\n\n\nAggregazione mensile\nPer rendere i dati più leggibili, aggreghiamo le informazioni a livello mensile.\nIn particolare, per ciascun mese calcoliamo:\n\nil numero di campioni analizzati;\nil tempo mediano di analisi;\nun intervallo dei tempi di analisi, escludendo i casi estremamente veloci e quelli estremamente lenti (10°–90° percentile).\n\nQuesto approccio consente di descrivere l’andamento tipico dei tempi di analisi senza essere troppo influenzati da valori anomali.\n\n# riassumo i tempi mensili di modo da ottenere:\n# - il numero di campioni;\n# - il tempo di analisi mediano\n# - il tempo di analisi dell'80% dei campioni, \n#   escludendo il 10% dei campioni velocissimi e \n#   il 10% dei campioni lentissimi\n\nriassunto &lt;- misure[id_metodo == params$method,\n              .(\n                campioni = uniqueN(id_campione),\n                analisi_mediana = median(tempo_analisi |&gt; as.numeric()),\n                analisi_veloce = quantile(tempo_analisi, 0.10),\n                analisi_lenta = quantile(tempo_analisi, 0.90)\n                ),\n              by = .(mese_inizio_analisi)]\n\n# i mesi non sono semplici numeri,\n# sono elementi di un fattore ordinato\nriassunto[, mese_inizio_analisi := factor(mese_inizio_analisi, \n                                          levels = 1:12,\n                                          labels = month.abb)]\n\n\n\nVisualizzazione dei risultati\nProseguiamo l’esercitazione con una rappresentazione grafica dei risultati:\n\nil primo grafico mostra l’andamento mensile del numero di campioni;\nil secondo grafico riassume la distribuzione dei tempi di analisi tramite mediana e percentili.\n\nLa combinazione dei due grafici permette di valutare visivamente se esiste una relazione tra carico di lavoro e tempi di analisi, e di individuare eventuali periodi critici dell’anno.\nÈ importante ricordare che i grafici mostrano andamenti aggregati: eventuali anomalie o picchi andrebbero approfonditi tornando ai dati di dettaglio.\n\nn_campioni &lt;- ggplot(riassunto,\n                     aes(x = mese_inizio_analisi,\n                         y = campioni)) +\n              geom_point() +\n              scale_y_continuous(breaks = function(x) floor(pretty(x))) +\n              labs(x = \"Mese di inizio analisi\",\n                   y = \"Numero di campioni analizzati\") +\n              theme_minimal()\n\ntempi &lt;- ggplot(riassunto,\n                aes(x = mese_inizio_analisi,\n                    ymin = analisi_veloce,\n                    ymax = analisi_lenta,\n                    y = analisi_mediana)) +\n         geom_pointrange() +\n         geom_hline(yintercept = params$target, col = \"red\", linetype = \"dashed\") +\n         scale_y_continuous(breaks = function(x) floor(pretty(x))) +\n         labs(x = \"Mese di inizio analisi\",\n              y = \"Tempo di analisi in giorni\\n(10°–50°–90° percentile)\") +\n         theme_minimal()\n\nn_campioni / tempi + plot_layout(axes = \"collect\")\n\n\n\n\n\n\n\nFigura 1: Andamento mensile del numero di campioni e tempi di analisi\n\n\n\n\n\nPosso aggiungere anche del testo contenente dei calcoli in linea. Per esempio, per il metodo CHEM5421c, nel 2024 il numero dei campioni con tempi di analisi superiori all’obiettivo di 15 giorni è stato 8, su un totale di 124 campioni analizzati.\n\n\nUnire più fonti di dati\nSpesso non tutti i dati che ci servono sono nello stesso file.\nNel nostro caso abbiamo caricato all’inizio dell’esercitazione elenco_comuni dove è disponibile la corrispondenza tra punti di campionamento e comuni. Combiniamo queste informazioni attraverso la funzione merge di data.table. In altri linguaggi come SQL, lo stesso concetto compare sotto il termine join.\n\nmisure_completo &lt;- merge(misure,                           # primo dataset\n                         elenco_comuni,                    # secondo dataset\n                         # sono in relazione attraverso una colonna che\n                         # ha gli stessi valori in entrambi i dataset,\n                         # ma ha nomi diversi\n                         by.x = \"punto_di_campionamento\",  # nome nel primo dataset\n                         by.y = \"punti_di_misura\")         # nome nel secondo dataset\n\nQuesto tipo di operazione è spesso critica in quanto si possono perdere o moltiplicare righe. Verifichiamo sempre con nrow() il numero di righe dei dataset di partenza e quelle del dataset unito.\nIn questo caso il numero di righe del dataset unito (misure_completo) combacia con quelle del dataset misure, mentre è molto maggiore del numero di righe del dataset elenco_comuni: alcuni punti di campionamento saranno stati indagati con più campioni.\nUn’altra verifica spesso importante è che non ci siano punti di campionamento non associati a campioni o campioni non associati a un punto di campionamento.\n\n\nTabelle\nA questo punto possiamo riassumere il numero di campioni indagati per ogni comune, il numero di misure con esito non conforme e il numero di misure per il quale non è stato possibile consegnare un risultato a causa di problemi tecnici all’interno del laboratorio.\n\nriassunto_comune &lt;- misure_completo[id_metodo == params$method,\n                    .(\n                      # questo è il numero di campioni\n                      campioni_analizzati= uniqueN(id_campione),\n                      # questo è il numero di misure non conformi\n                      misure_non_conformi = sum(!conforme, na.rm = TRUE),\n                      # questo è il numero di misure fallite\n                      misure_fallite = sum(is.na(valore))\n                    ),\n                    by = denominazione_ita]\n\n# cambio il nome delle colonne\nriassunto_comune[, `:=` (\n  comune = denominazione_ita,\n  denominazione_ita = NULL\n)]\n\n# riordino le colonne\nriassunto_comune &lt;- riassunto_comune[order(-campioni_analizzati), \n                      .(\n                        comune, \n                        campioni_analizzati,\n                        misure_non_conformi,\n                        misure_fallite\n                       )]\n\nPresento il risultato sotto forma di tabella.\n\nkable(riassunto_comune,\n        col.names = c(\"Comune\",\n                      \"Campioni analizzati\",\n                      \"Misure non conformi\",\n                      \"Misure fallite\"))\n\n\n\nTabella 1: Riassunto dei campioni analizzati, delle misure non conformi e fallite per ogni comune.\n\n\n\n\n\n\nComune\nCampioni analizzati\nMisure non conformi\nMisure fallite\n\n\n\n\nAlessandria\n124\n10\n0"
  }
]